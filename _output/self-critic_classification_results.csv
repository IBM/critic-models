model_name,learning_rate,batch_size,num_epochs,weight_decay,lora_r,lora_alpha,lora_dropout,seed,accuracy,f1
google/gemma-2-2b-it,1e-06,2,5,0.1,8,16,0.1,42,0.7462422634836428,0.5577812018489985
google/gemma-2-2b-it,3e-06,2,5,0.01,8,16,0.1,42,0.7656940760389036,0.567699836867863
google/gemma-2-2b-it,1e-06,2,5,0.01,8,16,0.1,42,0.7480106100795756,0.5601851851851852
google/gemma-2-2b-it,1e-05,2,5,0.01,8,16,0.1,42,0.7900088417329797,0.6141348497156783
google/gemma-2-2b-it,1e-05,2,5,0.1,8,16,0.1,42,0.7891246684350133,0.611879576891782
google/gemma-2-2b-it,3e-05,2,5,0.1,8,16,0.1,42,0.8050397877984085,0.6297229219143576
google/gemma-2-2b-it,0.0001,2,5,0.01,8,16,0.1,42,0.7970822281167109,0.6240786240786241
google/gemma-2-2b-it,0.0001,2,5,0.1,8,16,0.1,42,0.7979664014146772,0.6238683127572017
google/gemma-2-2b-it,3e-05,2,5,0.01,8,16,0.1,42,0.8059239610963749,0.628909551986475
